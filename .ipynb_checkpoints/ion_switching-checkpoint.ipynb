{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from functools import partial\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "def MacroF1MetricClassification(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = np.argmax(preds).astype(np.int16)\n",
    "    score = f1_score(labels, preds, average='macro')\n",
    "    return ('MacroF1Metric', score, True)\n",
    "\n",
    "\n",
    "def MacroF1MetricRegression(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = np.round(np.clip(preds, 0, 10)).astype(np.int16)\n",
    "    score = f1_score(labels, preds, average='macro')\n",
    "    return ('MacroF1Metric', score, True)\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2  # just added\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(\n",
    "                        np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(\n",
    "                        np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(\n",
    "                        np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(\n",
    "                        np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(\n",
    "                        np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(\n",
    "                        np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    percent = 100 * (start_mem - end_mem) / start_mem\n",
    "    print(\n",
    "        'Mem. usage decreased from {:5.2f} Mb to {:5.2f} Mb ({:.1f}% reduction)'\n",
    "        .format(start_mem, end_mem, percent))\n",
    "    return df\n",
    "\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    \"\"\"\n",
    "    An optimizer for rounding thresholds\n",
    "    to maximize F1 (Macro) score\n",
    "    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _f1_loss(self, coef, X, y):\n",
    "        \"\"\"\n",
    "        Get loss according to\n",
    "        using current coefficients\n",
    "\n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf],\n",
    "                     labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "        return -f1_score(y, X_p, average='macro')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Optimize rounding thresholds\n",
    "\n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        loss_partial = partial(self._f1_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial,\n",
    "                                          initial_coef,\n",
    "                                          method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \"\"\"\n",
    "        Make predictions with specified thresholds\n",
    "\n",
    "        :param X: The raw predictions\n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        \"\"\"\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf],\n",
    "                      labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "    def coefficients(self):\n",
    "        \"\"\"\n",
    "        Return the optimized coefficients\n",
    "        \"\"\"\n",
    "        return self.coef_['x']\n",
    "\n",
    "\n",
    "def cross_validate(params: dict,\n",
    "                   X,\n",
    "                   y,\n",
    "                   X_test,\n",
    "                   oof_df,\n",
    "                   features,\n",
    "                   model_type: str = 'lgb',\n",
    "                   feval: dict = {},\n",
    "                   objective: str = 'regression',\n",
    "                   num_boost_round: int = 1,\n",
    "                   early_stopping_rounds: int = 50,\n",
    "                   sklearn_model = None):\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    feat_importance_df = pd.DataFrame(index=features)\n",
    "    fold = 0\n",
    "    if model_type == 'lgb':\n",
    "        for train_id, valid_id in kfold.split(X, y):\n",
    "            fold += 1\n",
    "            x_train, y_train = X.iloc[train_id, :], y[train_id]\n",
    "            x_val, y_val = X.iloc[valid_id, :], y[valid_id]\n",
    "\n",
    "            train_set = lgb.Dataset(x_train, y_train)\n",
    "            valid_set = lgb.Dataset(x_val, y_val)\n",
    "\n",
    "            model = lgb.train(params=params,\n",
    "                              feval=feval[objective],\n",
    "                              train_set=train_set,\n",
    "                              num_boost_round=num_boost_round,\n",
    "                              early_stopping_rounds=early_stopping_rounds,\n",
    "                              valid_sets=[train_set, valid_set],\n",
    "                              verbose_eval=1000)\n",
    "            if objective == 'regression':\n",
    "                pred = model.predict(x_val, num_iteration=model.best_iteration)\n",
    "                pred = np.round(np.clip(pred, 0, 10)).astype(np.int32)\n",
    "                test_preds = model.predict(X_test,\n",
    "                                           num_iteration=model.best_iteration)\n",
    "                test_preds = np.round(np.clip(test_preds, 0,\n",
    "                                              10)).astype(np.int32)\n",
    "            elif objective == 'classification':\n",
    "                pred = model.predict(x_val, num_iteration=model.best_iteration)\n",
    "                pred = np.argmax(pred).astype(np.int32)\n",
    "                test_preds = model.predict(X_test,\n",
    "                                           num_iteration=model.best_iteration)\n",
    "                test_preds = np.argmax(test_preds, 0, 10).astype(np.int32)\n",
    "\n",
    "            oof_df.loc[oof_df.iloc[valid_id].index, 'oof'] = pred\n",
    "            sub[f'{model_type}_open_channels_fold_{fold}'] = test_preds\n",
    "\n",
    "            f1 = f1_score(\n",
    "                oof_df.loc[oof_df.iloc[valid_id].index]['open_channels'],\n",
    "                oof_df.loc[oof_df.iloc[valid_id].index]['oof'],\n",
    "                average='macro')\n",
    "            rmse = np.sqrt(\n",
    "                mean_squared_error(\n",
    "                    oof_df.loc[oof_df.index.isin(valid_id)]['open_channels'],\n",
    "                    oof_df.loc[oof_df.index.isin(valid_id)]['oof']))\n",
    "            feat_importance_df[\n",
    "                f'{model_type}_importance_{fold}'] = model.feature_importance(\n",
    "                )\n",
    "\n",
    "        oof_f1 = f1_score(oof_df['open_channels'],\n",
    "                          oof_df['oof'],\n",
    "                          average='macro')\n",
    "        oof_rmse = np.sqrt(\n",
    "            mean_squared_error(oof_df['open_channels'], oof_df['oof']))\n",
    "    elif model_type == 'xgb':\n",
    "        test_set = xgb.DMatrix(X_test)\n",
    "        for train_id, valid_id in kfold.split(X, y):\n",
    "            fold += 1\n",
    "            x_train, y_train = X.iloc[train_id, :], y[train_id]\n",
    "            x_val, y_val = X.iloc[valid_id, :], y[valid_id]\n",
    "\n",
    "            train_set = xgb.DMatrix(x_train, y_train)\n",
    "            valid_set = xgb.DMatrix(x_val, y_val)\n",
    "\n",
    "            model = xgb.train(params=params,\n",
    "                              dtrain=train_set,\n",
    "                              num_boost_round=num_boost_round,\n",
    "                              early_stopping_rounds=early_stopping_rounds,\n",
    "                              evals=((train_set, 'train'), (valid_set, 'val')),\n",
    "                              verbose_eval=1000)\n",
    "            if objective == 'regression':\n",
    "                pred = model.predict(x_val, ntree_limit=model.best_ntree_limit)\n",
    "                #pred = np.round(np.clip(pred, 0, 10)).astype(np.int32)\n",
    "                test_preds = model.predict(test_set,\n",
    "                                           ntree_limit=model.best_ntree_limit)\n",
    "#                 test_preds = np.round(np.clip(test_preds, 0,\n",
    "#                                               10)).astype(np.int32)\n",
    "            elif objective == 'classification':\n",
    "                pred = model.predict(x_val, ntree_limit=model.best_ntree_limit)\n",
    "                pred = np.argmax(pred).astype(np.int16)\n",
    "                test_preds = model.predict(test_set,\n",
    "                                           ntree_limit=model.best_ntree_limit)\n",
    "                test_preds = np.argmax(test_preds).astype(np.int32)\n",
    "\n",
    "            oof_df.loc[oof_df.iloc[valid_id].index, 'oof'] = pred\n",
    "            sub[f'{model_type}_open_channels_fold_{fold}'] = test_preds\n",
    "\n",
    "            f1 = f1_score(\n",
    "                oof_df.loc[oof_df.iloc[valid_id].index]['open_channels'],\n",
    "                oof_df.loc[oof_df.iloc[valid_id].index]['oof'],\n",
    "                average='macro')\n",
    "            rmse = np.sqrt(\n",
    "                mean_squared_error(\n",
    "                    oof_df.loc[oof_df.index.isin(valid_id)]['open_channels'],\n",
    "                    oof_df.loc[oof_df.index.isin(valid_id)]['oof'])).astype(\n",
    "                        np.float32)\n",
    "\n",
    "            feat_importance_df[\n",
    "                f'{model_type}_importance_{fold}'] = model.feature_importances_\n",
    "\n",
    "        oof_f1 = f1_score(oof_df['open_channels'],\n",
    "                          oof_df['oof'],\n",
    "                          average='macro')\n",
    "        oof_rmse = np.sqrt(\n",
    "            mean_squared_error(oof_df['open_channels'], oof_df['oof']))\n",
    "    elif model_type == 'sklearn':\n",
    "        rounder = OptimizedRounder()\n",
    "        for train_id, valid_id in kfold.split(X, y):\n",
    "            fold += 1\n",
    "            x_train, y_train = X.iloc[train_id, :], y[train_id]\n",
    "            x_val, y_val = X.iloc[valid_id, :], y[valid_id]\n",
    "            \n",
    "            model = sklearn_model.fit(x_train, y_train)\n",
    "            pred = model.predict(x_val)\n",
    "            rounder.fit(X=y_val, y=pred)\n",
    "            test_preds = rounder.predict(X=model.predict(X_test),\n",
    "                                         coef=rounder.coefficients())\n",
    "            oof_df.loc[oof_df.iloc[valid_id].index, 'oof'] = rounder.predict(pred, \n",
    "                                                                             rounder.coefficients())\n",
    "            sub[f'{model_type}_open_channels_fold_{fold}'] = test_preds\n",
    "            f1 = f1_score(\n",
    "                oof_df.loc[oof_df.iloc[valid_id].index]['open_channels'],\n",
    "                oof_df.loc[oof_df.iloc[valid_id].index]['oof'],\n",
    "                average='macro')\n",
    "            rmse = np.sqrt(\n",
    "                mean_squared_error(\n",
    "                    oof_df.loc[oof_df.index.isin(valid_id)]['open_channels'],\n",
    "                    oof_df.loc[oof_df.index.isin(valid_id)]['oof'])).astype(\n",
    "                        np.float32)\n",
    "        oof_f1 = f1_score(oof_df['open_channels'],\n",
    "                          oof_df['oof'],\n",
    "                          average='macro')\n",
    "        oof_rmse = np.sqrt(\n",
    "            mean_squared_error(oof_df['open_channels'], oof_df['oof']))\n",
    "        \n",
    "    return oof_df.copy(), feat_importance_df.copy(), sub.copy(), oof_f1, oof_rmse\n",
    "\n",
    "\n",
    "feval = {\n",
    "    'classification': MacroF1MetricClassification,\n",
    "    'regression': MacroF1MetricRegression\n",
    "}\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('liverpool-ion-switching/train.csv',\n",
    "                    dtype={\n",
    "                        'time': np.float32,\n",
    "                        'signal': np.float32,\n",
    "                        'open_channels': np.int32\n",
    "                    })\n",
    "test = pd.read_csv('liverpool-ion-switching/test.csv',\n",
    "                    dtype={\n",
    "                        'time': np.float32,\n",
    "                        'signal': np.float32,\n",
    "                    })\n",
    "sub = pd.read_csv('liverpool-ion-switching/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df: pd.DataFrame, bs=500_000, bs_slice=25_000):\n",
    "    df = df.sort_values(by=['time']).reset_index(drop=True)\n",
    "    df.index = ((df.time * 10_000) - 1).values\n",
    "    df['batch'] = df.index // bs\n",
    "    df['batch_index'] = df.index - (df.batch * bs)\n",
    "    df['batch_slices'] = df['batch_index'] // bs_slice\n",
    "    df['batch_slices2'] = df['batch'].astype(str).str.zfill(\n",
    "        3) + '_' + df['batch_slices'].astype(str).str.zfill(3)\n",
    "\n",
    "    for c in ['batch', 'batch_slices2']:\n",
    "        df[f'batch_{bs//1000}k_max_{c}'] = df.groupby(\n",
    "            [f'{c}'])['signal'].transform(np.max)\n",
    "        df[f'batch_{bs//1000}k_min_{c}'] = df.groupby(\n",
    "            [f'{c}'])['signal'].transform(np.min)\n",
    "        df[f'batch_{bs//1000}k_mean_{c}'] = df.groupby(\n",
    "            [f'{c}'])['signal'].transform(np.mean)\n",
    "        df[f'batch_{bs//1000}k_std_{c}'] = df.groupby(\n",
    "            [f'{c}'])['signal'].transform(np.std)\n",
    "        df[f'batch_{bs//1000}k_median_{c}'] = df.groupby(\n",
    "            [f'{c}'])['signal'].transform(np.median)\n",
    "        df[f'batch_{bs//1000}k_diff_max_{c}'] = df.groupby(\n",
    "            [f'{c}'])['signal'].transform(lambda x: np.max(np.diff(x)))\n",
    "        df[f'batch_{bs//1000}k_diff_min_{c}'] = df.groupby(\n",
    "            [f'{c}'])['signal'].transform(lambda x: np.min(np.diff(x)))\n",
    "        df[f'batch_{bs//1000}k_range_{c}'] = np.abs(\n",
    "            df[f'batch_{bs//1000}k_max_{c}'] - df[f'batch_{bs//1000}k_min_{c}'])\n",
    "        df[f'batch_{bs//1000}k_maxtomin_{c}'] = np.abs(\n",
    "            (df[f'batch_{bs//1000}k_max_{c}'] + 1e-8) /\n",
    "            (df[f'batch_{bs//1000}k_min_{c}'] + 1e-8))\n",
    "        \n",
    "        df[f'batch_{bs//1000}k_shift_1_{c}'] = df.groupby(\n",
    "            [f'{c}']).shift(1)['signal']\n",
    "        df[f'batch_{bs//1000}k_shift_-1_{c}'] = df.groupby(\n",
    "            [f'{c}']).shift(-1)['signal']\n",
    "        df[f'batch_{bs//1000}k_shift_2_{c}'] = df.groupby(\n",
    "            [f'{c}']).shift(2)['signal']\n",
    "        df[f'batch_{bs//1000}k_shift_-2_{c}'] = df.groupby(\n",
    "            [f'{c}']).shift(-2)['signal']\n",
    "    feats = [\n",
    "        c for c in df.columns if c not in [\n",
    "            'time', 'open_channels', 'batch', 'batch_index', 'batch_slices',\n",
    "            'batch_slices2'\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    for c in feats + ['signal']:\n",
    "        df[c + '_msignal'] = df[c] - df['signal']\n",
    "\n",
    "    return df, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased from 1606.46 Mb to 905.51 Mb (43.6% reduction)\n",
      "Mem. usage decreased from 637.05 Mb to 362.40 Mb (43.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "train, features = feature_eng(train, bs=50_000, bs_slice=5000)\n",
    "train = reduce_mem_usage(train)\n",
    "test, _ = feature_eng(test, bs=50_000, bs_slice=5000)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features]\n",
    "X_test = test[features]\n",
    "y = train['open_channels']\n",
    "oof_df = train[['time', 'open_channels']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Signal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy import signal as sps\n",
    "from numpy import fft\n",
    "import pywt\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "signal = train.signal.values\n",
    "time = train.time.values\n",
    "\n",
    "\n",
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p = figure(plot_width=800, plot_height=400)\n",
    "\n",
    "p.line(train.time.values[::10000], signal[::10000])\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LightGBM`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X,\n",
    "                                                  y,\n",
    "                                                  test_size=0.2,\n",
    "                                                  stratify=y)\n",
    "train_set = lgb.Dataset(x_train, y_train)\n",
    "val_set = lgb.Dataset(x_val, y_val)\n",
    "\n",
    "# below are fixed parameters for which we don't really need anything\n",
    "RANDOM_SEED = 42\n",
    "MODEL_TYPE = 'LGBM'\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "N_THREADS = -1\n",
    "OBJECTIVE = 'regression'\n",
    "NUM_CLASS = 1 if OBJECTIVE == 'regression' else 11\n",
    "METRIC = 'rmse'\n",
    "NUM_BOOST_ROUND = 250_000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `max_depth` and `min_child_weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DEPTH = -1\n",
    "MIN_CHILD_WEIGHT = 0\n",
    "LEARNING_RATE = 0.09\n",
    "NUM_LEAVES = 2**8 + 1\n",
    "FEATURE_FRACTION = 1\n",
    "BAGGING_FRACTION = 1\n",
    "BAGGING_FREQ = 0\n",
    "L1 = 0\n",
    "L2 = 0\n",
    "params_lgb = {\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'lambda_l1': L1,\n",
    "    'lambda_l2': L2,\n",
    "    'max_depth': MAX_DEPTH,\n",
    "    'min_child_weight': MIN_CHILD_WEIGHT,\n",
    "    'num_leaves': NUM_LEAVES,\n",
    "    'feature_fraction': FEATURE_FRACTION,\n",
    "    'bagging_fraction': BAGGING_FRACTION,\n",
    "    'bagging_freq': BAGGING_FREQ,\n",
    "    'n_jobs': N_THREADS,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'metric': METRIC,\n",
    "    'objective': OBJECTIVE,\n",
    "    'num_class': NUM_CLASS,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [(max_depth, min_child_weight)\n",
    "                    for max_depth in np.arange(-1, 12, 4)\n",
    "                    for min_child_weight in np.arange(1e-3, 0.1, 0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=-1, min_child_weight=0.001\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'test-rmse-mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-bffd28d6bfd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Update best MAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmean_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test-rmse-mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mboost_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test-rmse-mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\tRMSE {mean_rmse} for {boost_rounds} rounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test-rmse-mean'"
     ]
    }
   ],
   "source": [
    "min_rmse = np.float32(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(f\"CV with max_depth={max_depth}, min_child_weight={min_child_weight}\")\n",
    "    # Update our parameters\n",
    "    params_lgb['max_depth'] = max_depth\n",
    "    params_lgb['min_child_weight'] = min_child_weight    # Run CV\n",
    "    cv_results = lgb.cv(\n",
    "        params_lgb,\n",
    "        train_set,\n",
    "        num_boost_round=NUM_BOOST_ROUND,\n",
    "        seed=RANDOM_SEED,\n",
    "        nfold=5,\n",
    "        metrics={METRIC},\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS\n",
    "    )\n",
    "        \n",
    "    mean_rmse = cv_results['rmse-mean'].min()\n",
    "    boost_rounds = cv_results['rmse-mean'].argmin()\n",
    "    print(f\"\\tRMSE {mean_rmse} for {boost_rounds} rounds\")\n",
    "    if mean_rmse < min_rmse:\n",
    "        min_rmse = mean_rmse\n",
    "        best_params = (max_depth, min_child_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb['max_depth'], params_lgb['min_child_weight'] = best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `num_leaves`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rmse = np.float32(\"Inf\")\n",
    "best_params = None\n",
    "gridsearch_params = [num_leaves for num_leaves in range(64, 2**10+1, 64)]\n",
    "for num_leaves in gridsearch_params:\n",
    "    print(f\"CV with num_leaves = {num_leaves}\")\n",
    "    # Update our parameters\n",
    "    params_lgb['num_leaves'] = num_leaves    # Run CV\n",
    "    cv_results = lgb.cv(\n",
    "        params_lgb,\n",
    "        train_set,\n",
    "        num_boost_round=NUM_BOOST_ROUND,\n",
    "        seed=RANDOM_SEED,\n",
    "        nfold=5,\n",
    "        metrics={METRIC},\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS\n",
    "    )\n",
    "        # Update best MAE\n",
    "    mean_rmse = cv_results['test-rmse-mean'].min()\n",
    "    boost_rounds = cv_results['test-rmse-mean'].argmin()\n",
    "    print(f\"\\tRMSE {mean_rmse} for {boost_rounds} rounds\")\n",
    "    if mean_rmse < min_rmse:\n",
    "        min_rmse = mean_rmse\n",
    "        best_params = num_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb['num_leaves'] = best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `feature_frac`, `baggin_frac`, `bagging_freq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rmse = np.float32(\"Inf\")\n",
    "best_params = None\n",
    "gridsearch_params = [(feature_frac, baggin_frac, bagging_freq) \n",
    "                     for feature_frac in np.arange(0., 1., 0.2)\n",
    "                     for baggin_frac in np.arange(0., 1., 0.2)\n",
    "                     for baggin_freq in np.arange(0, 150, 50)]\n",
    "for feature_frac, baggin_frac, bagging_freq in gridsearch_params:\n",
    "    print(f\"CV with feature_frac = {feature_frac}, baggin_frac={baggin_frac}, bagging_freq={bagging_freq}\")\n",
    "    # Update our parameters\n",
    "    params_lgb['feature_frac'] = feature_frac    # Run CV\n",
    "    params_lgb['baggin_frac'] = baggin_frac    # Run CV\n",
    "    params_lgb['baggin_freq'] = baggin_freq    # Run CV\n",
    "    cv_results = lgb.cv(\n",
    "        params_lgb,\n",
    "        train_set,\n",
    "        num_boost_round=NUM_BOOST_ROUND,\n",
    "        seed=RANDOM_SEED,\n",
    "        nfold=5,\n",
    "        metrics={METRIC},\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS\n",
    "    )\n",
    "        # Update best MAE\n",
    "    mean_rmse = cv_results['test-rmse-mean'].min()\n",
    "    boost_rounds = cv_results['test-rmse-mean'].argmin()\n",
    "    print(f\"\\tRMSE {mean_rmse} for {boost_rounds} rounds\")\n",
    "    if mean_rmse < min_rmse:\n",
    "        min_rmse = mean_rmse\n",
    "        best_params = (feature_frac, baggin_frac, bagging_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb['feature_frac'], params_lgb['baggin_frac'], params_lgb['baggin_freq'] = best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `learning_rate`, `L1`, `L2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rmse = np.float32(\"Inf\")\n",
    "best_params = None\n",
    "gridsearch_params = [(learning_rate, lambda_l1, lambda_l2) \n",
    "                     for learning_rate in [0.3, 0.2, 0.1, 0.05, 0.01, 0.05]\n",
    "                     for lambda_l1 in [0.3, 0.2, 0.1, 0.05, 0.01, 0.05]\n",
    "                     for lambda_l2 in [0.3, 0.2, 0.1, 0.05, 0.01, 0.05]]\n",
    "for learning_rate, lambda_l1, lambda_l2 in gridsearch_params:\n",
    "    print(f\"CV with learning_rate = {learning_rate}, lambda_l1={lambda_l1}, lambda_l2={lambda_l2}\")\n",
    "    # Update our parameters\n",
    "    params_lgb['learning_rate'] = learning_rate    # Run CV\n",
    "    params_lgb['lambda_l1'] = lambda_l1    # Run CV\n",
    "    params_lgb['lambda_l2'] = lambda_l2    # Run CV\n",
    "    cv_results = lgb.cv(\n",
    "        params_lgb,\n",
    "        train_set,\n",
    "        num_boost_round=NUM_BOOST_ROUND,\n",
    "        seed=RANDOM_SEED,\n",
    "        nfold=5,\n",
    "        metrics={METRIC},\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS\n",
    "    )\n",
    "        # Update best MAE\n",
    "    mean_rmse = cv_results['test-rmse-mean'].min()\n",
    "    boost_rounds = cv_results['test-rmse-mean'].argmin()\n",
    "    print(f\"\\tRMSE {mean_rmse} for {boost_rounds} rounds\")\n",
    "    if mean_rmse < min_rmse:\n",
    "        min_rmse = mean_rmse\n",
    "        best_params = (learning_rate, lambda_l1, lambda_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb['learning_rate'], params_lgb['lambda_l1'], params_lgb['lambda_l2'] = best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using learned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[947]\ttraining's rmse: 0.16443\ttraining's MacroF1Metric: 0.929394\tvalid_1's rmse: 0.166628\tvalid_1's MacroF1Metric: 0.928888\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[1000]\ttraining's rmse: 0.163996\ttraining's MacroF1Metric: 0.929733\tvalid_1's rmse: 0.16699\tvalid_1's MacroF1Metric: 0.928086\n",
      "Early stopping, best iteration is:\n",
      "[1256]\ttraining's rmse: 0.163\ttraining's MacroF1Metric: 0.930483\tvalid_1's rmse: 0.166604\tvalid_1's MacroF1Metric: 0.928432\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[1000]\ttraining's rmse: 0.164024\ttraining's MacroF1Metric: 0.929503\tvalid_1's rmse: 0.16682\tvalid_1's MacroF1Metric: 0.928063\n",
      "Early stopping, best iteration is:\n",
      "[976]\ttraining's rmse: 0.164143\ttraining's MacroF1Metric: 0.929461\tvalid_1's rmse: 0.166876\tvalid_1's MacroF1Metric: 0.928086\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[1000]\ttraining's rmse: 0.163975\ttraining's MacroF1Metric: 0.929679\tvalid_1's rmse: 0.166867\tvalid_1's MacroF1Metric: 0.927972\n",
      "Early stopping, best iteration is:\n",
      "[1255]\ttraining's rmse: 0.162986\ttraining's MacroF1Metric: 0.930445\tvalid_1's rmse: 0.166522\tvalid_1's MacroF1Metric: 0.92817\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[869]\ttraining's rmse: 0.164876\ttraining's MacroF1Metric: 0.928811\tvalid_1's rmse: 0.167105\tvalid_1's MacroF1Metric: 0.927412\n"
     ]
    }
   ],
   "source": [
    "oof_df_lgb, feat_importance_df_lgb, sub_lgb = cross_validate(\n",
    "    params = params_lgb,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    X_test=X_test,\n",
    "    oof_df=oof_df,\n",
    "    features=features,\n",
    "    feval=feval,\n",
    "    model_type='lgb',\n",
    "    objective=OBJECTIVE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_df_lgb.to_csv('oof_df_lgb.csv', index=False)\n",
    "feat_importance_df_lgb.to_csv('feat_importance_df_lgb.csv', index=False)\n",
    "sub_lgb.to_csv('sub_lgb_with_folds.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `XGBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'open_channels'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "MODEL_TYPE = 'LGBM'\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_BOOST_ROUND = 5000\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "N_THREADS = -1\n",
    "OBJECTIVE = 'reg:squarederror'\n",
    "METRIC = 'rmse'\n",
    "MAX_DEPTH = 10\n",
    "L1 = 0\n",
    "L2 = 0\n",
    "\n",
    "params_xgb = {\n",
    "    'colsample_bytree': 0.375,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'max_depth': MAX_DEPTH,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'eval_metric': METRIC,\n",
    "    'objective': OBJECTIVE,\n",
    "    'subsample': 1,\n",
    "    'reg_lambda': L2,\n",
    "    'reg_alpha': L1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:3.1386\tval-rmse:3.13293\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 250 rounds.\n",
      "[1000]\ttrain-rmse:0.136969\tval-rmse:0.164947\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-30d3a45390f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOBJECTIVE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_BOOST_ROUND\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     early_stopping_rounds=EARLY_STOPPING_ROUNDS)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-b45f92ae4dbe>\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(params, X, X_test, y, oof_df, features, model_type, feval, objective, num_boost_round, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    197\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                               \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                               verbose_eval=1000)\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mobjective\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'regression'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_ntree_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oof_df_xgb, feat_importance_df_xgb, sub_xgb = cross_validate(\n",
    "    X=X.reset_index(drop=True),\n",
    "    y=y.reset_index(drop=True),\n",
    "    X_test=X_test.reset_index(drop=True),\n",
    "    oof_df=oof_df,\n",
    "    features=features,\n",
    "    params = params_xgb,\n",
    "    model_type='xgb',\n",
    "    objective=OBJECTIVE,\n",
    "    num_boost_round=NUM_BOOST_ROUND,\n",
    "    early_stopping_rounds=EARLY_STOPPING_ROUNDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SKLearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## `PyTorch` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cols = [s for s in sub.columns if 'open_channels' in s]\n",
    "\n",
    "sub['open_channels'] = sub[s_cols].median(axis=1).astype(int)\n",
    "\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
